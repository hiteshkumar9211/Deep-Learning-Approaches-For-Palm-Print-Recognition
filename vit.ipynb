{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1303db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "from torchvision import transforms, models\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0a05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loader for Tonji\n",
    "class PalmROIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.img_list = sorted([f for f in os.listdir(root_dir) if f.endswith(\".bmp\")])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "        image = cv2.resize(image, (224, 224))  # GoogLeNet expects 224x224 input\n",
    "        image = image[..., np.newaxis]\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Extract person ID\n",
    "        try:\n",
    "            img_number = int(img_name.split(\".\")[0])\n",
    "            person_id = ((img_number - 1) // 20) + 1\n",
    "            label = person_id - 1\n",
    "            if not (0 <= label <= 299):\n",
    "                raise ValueError(f\"Label {label + 1} out of range (1 to 300) for {img_name}\")\n",
    "        except (ValueError, IndexError):\n",
    "            raise ValueError(f\"Invalid filename format for label: {img_name}\")\n",
    "        return image, label, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ad4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ViTFeature(nn.Module):\n",
    "    def __init__(self, num_classes=300):\n",
    "        super(ViTFeature, self).__init__()\n",
    "        # Load pretrained Vision Transformer (ViT-B/16)\n",
    "        self.vit = models.vit_b_16(weights='IMAGENET1K_V1')\n",
    "        # Average pretrained patch embedding conv weights for grayscale adaptation\n",
    "        with torch.no_grad():\n",
    "            pretrained_weight = self.vit.conv_proj.weight  # [768, 3, 16, 16]\n",
    "            averaged_weight = pretrained_weight.mean(dim=1, keepdim=True)  # [768, 1, 16, 16]\n",
    "            replicated_weight = averaged_weight.repeat(1, 3, 1, 1)  # [768, 3, 16, 16]\n",
    "            self.vit.conv_proj.weight.copy_(replicated_weight)\n",
    "        # Store components separately for proper forward pass\n",
    "        self.conv_proj = self.vit.conv_proj  # Patch embedding convolution\n",
    "        self.encoder = self.vit.encoder  # Transformer encoder\n",
    "        self.pos_embedding = self.vit.encoder.pos_embedding  # Positional embeddings\n",
    "        self.cls_token = self.vit.class_token  # Class token\n",
    "        # Dropout for consistency\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # Custom classifier (768 features to 300 classes)\n",
    "        self.classifier = nn.Linear(768, num_classes)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, x, return_features=False):\n",
    "        # Replicate 1-channel to 3 channels\n",
    "        x = x.repeat(1, 3, 1, 1)  \n",
    "        # Patch embedding\n",
    "        x = self.conv_proj(x)  \n",
    "        # Flatten patches to sequence\n",
    "        batch_size = x.size(0)\n",
    "        x = x.flatten(2).transpose(1, 2)  \n",
    "        # Add class token\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)  \n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embedding  # \n",
    "        # Pass through encoder\n",
    "        x = self.encoder(x)  \n",
    "        # Extract [CLS] token for features\n",
    "        x = x[:, 0]  \n",
    "        x = self.dropout(x)\n",
    "        if return_features:\n",
    "            return x  \n",
    "        # Apply classifier\n",
    "        x = self.classifier(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce890889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_vit(model, train_loader, device, epochs=40):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Optimizer with differential learning rates (no overlap)\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': list(model.conv_proj.parameters()) + list(model.encoder.parameters()), 'lr': 0.0001},  # Low LR for feature extraction\n",
    "        {'params': model.classifier.parameters(), 'lr': 0.001},  # High LR for classifier\n",
    "    ])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    model.train()\n",
    "    prev_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x, y, _ in tqdm(train_loader, desc=f\"Fine-Tuning Epoch {epoch+1}\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Fine-Tuning Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "        scheduler.step(avg_loss)\n",
    "        current_lr = [group['lr'] for group in optimizer.param_groups]\n",
    "        if any(lr != prev_lr[i] for i, lr in enumerate(current_lr)):\n",
    "            print(f\"Epoch {epoch+1}: Learning rates updated to {current_lr}\")\n",
    "        prev_lr = current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7be7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features\n",
    "def extract_features(model, dataloader, device):\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    img_names = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, names in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            x = x.to(device)\n",
    "            f = model(x, return_features=True).cpu().numpy()\n",
    "            features_list.append(f)\n",
    "            labels_list.extend(y.numpy())\n",
    "            img_names.extend(names)\n",
    "    features = np.concatenate(features_list, axis=0)\n",
    "    labels = np.array(labels_list)\n",
    "    return features, labels, img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2680c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def compute_metrics(features, labels, clf):\n",
    "    decision_scores = clf.decision_function(features)\n",
    "    topk_acc = {}\n",
    "    for k in [1, 5]:\n",
    "        top_k_indices = np.argsort(decision_scores, axis=1)[:, -k:]\n",
    "        correct = 0\n",
    "        for i, top_k in enumerate(top_k_indices):\n",
    "            if labels[i] in top_k:\n",
    "                correct += 1\n",
    "        topk_acc[k] = correct / len(labels)\n",
    "    y_true, y_score = [], []\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            score_i = decision_scores[i, labels[i]]\n",
    "            score_j = decision_scores[j, labels[j]]\n",
    "            sim = score_i + score_j\n",
    "            y_score.append(sim)\n",
    "            y_true.append(1 if labels[i] == labels[j] else 0)\n",
    "    if len(set(y_true)) <= 1:\n",
    "        print(\"Warning: y_true contains only one class. Setting ROC AUC, EER, FAR, FRR to 0.1.\")\n",
    "        roc_auc = 0.1\n",
    "        eer = 0.1\n",
    "        far = 0.1\n",
    "        frr = 0.1\n",
    "    else:\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        fnr = 1 - tpr\n",
    "        diff = np.abs(fnr - fpr)\n",
    "        if np.all(np.isnan(diff)):\n",
    "            print(\"Warning: All-NaN slice in EER calculation. Setting EER, FAR, FRR to 0.1.\")\n",
    "            eer = 0.1\n",
    "            far = 0.1\n",
    "            frr = 0.1\n",
    "        else:\n",
    "            eer_idx = np.nanargmin(diff)\n",
    "            eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "            far = fpr[eer_idx]\n",
    "            frr = fnr[eer_idx]\n",
    "    return topk_acc, roc_auc, eer, far, frr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d714a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code\n",
    "session1_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session1\"\n",
    "session2_root = r\"C:\\Users\\hiteshk\\Desktop\\Deep Learning Approaches for roi extraction and using same for palm print recognisation\\Tonji\\ROI\\session2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f82833",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02bc2533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for Tonji\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67069b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tongji datasets\n",
    "dataset1 = PalmROIDataset(session1_root, transform=transform)\n",
    "dataset2 = PalmROIDataset(session2_root, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0890a80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 images: 6000\n",
      "Session 2 images: 6000\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset sizes\n",
    "print(f\"Session 1 images: {len(dataset1)}\")\n",
    "print(f\"Session 2 images: {len(dataset2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3619f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 12000\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets\n",
    "full_dataset = ConcatDataset([dataset1, dataset2])\n",
    "print(f\"Total images: {len(full_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44cf3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test (80/20)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "415278aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b27f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 1: 100%|██████████| 300/300 [52:03<00:00, 10.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 1, Loss: 5.7689, Accuracy: 0.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 2: 100%|██████████| 300/300 [47:37<00:00,  9.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 2, Loss: 5.5448, Accuracy: 0.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 3: 100%|██████████| 300/300 [45:40<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 3, Loss: 5.4129, Accuracy: 1.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 4: 100%|██████████| 300/300 [45:32<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 4, Loss: 5.3284, Accuracy: 1.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 5: 100%|██████████| 300/300 [45:33<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 5, Loss: 5.1607, Accuracy: 2.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 6: 100%|██████████| 300/300 [45:38<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 6, Loss: 4.9671, Accuracy: 2.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 7: 100%|██████████| 300/300 [45:41<00:00,  9.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 7, Loss: 4.5431, Accuracy: 5.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 8: 100%|██████████| 300/300 [45:32<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 8, Loss: 3.7144, Accuracy: 15.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 9: 100%|██████████| 300/300 [45:38<00:00,  9.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 9, Loss: 2.6373, Accuracy: 34.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 10: 100%|██████████| 300/300 [45:35<00:00,  9.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 10, Loss: 1.6017, Accuracy: 57.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 11: 100%|██████████| 300/300 [45:30<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 11, Loss: 0.8700, Accuracy: 75.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 12: 100%|██████████| 300/300 [45:29<00:00,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 12, Loss: 0.4912, Accuracy: 85.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Epoch 13:  45%|████▍     | 134/300 [20:35<25:30,  9.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = ViTFeature(num_classes=\u001b[32m300\u001b[39m).to(device)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mfine_tune_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mfine_tune_vit\u001b[39m\u001b[34m(model, train_loader, device, epochs)\u001b[39m\n\u001b[32m     20\u001b[39m loss.backward()\n\u001b[32m     21\u001b[39m optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m _, predicted = torch.max(outputs, \u001b[32m1\u001b[39m)\n\u001b[32m     24\u001b[39m total += y.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = ViTFeature(num_classes=300).to(device)\n",
    "fine_tune_vit(model, train_loader, device, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0c6696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 300/300 [04:42<00:00,  1.06it/s]\n",
      "Extracting features: 100%|██████████| 75/75 [01:12<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "train_features, train_labels, _ = extract_features(model, train_loader, device)\n",
    "test_features, test_labels, _ = extract_features(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d03abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 300\n"
     ]
    }
   ],
   "source": [
    "# Verify number of classes\n",
    "num_classes = len(np.unique(np.concatenate((train_labels, test_labels))))\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "if num_classes != 300:\n",
    "    raise ValueError(f\"Expected 300 classes, but found {num_classes}. Verify filename format (e.g., '00001.bmp' to '12000.bmp').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "337c8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "train_features = train_features / np.linalg.norm(train_features, axis=1, keepdims=True)\n",
    "test_features = test_features / np.linalg.norm(test_features, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18dc2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with RBF kernel and hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21076af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM parameters: {'C': 10, 'gamma': 'scale'}\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC(kernel='rbf', decision_function_shape='ovr', probability=False)\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "print(f\"Best SVM parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e59acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best estimator\n",
    "svm_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b340781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "topk_acc, roc_auc, eer, far, frr = compute_metrics(test_features, test_labels, svm_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae17cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Identification Accuracy: 98.29%\n",
      "Top-5 Identification Accuracy: 99.58%\n",
      "ROC AUC: 0.4898\n",
      "EER: 0.5059\n",
      "FAR: 0.5059\n",
      "FRR: 0.5060\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "print(f\"Top-1 Identification Accuracy: {topk_acc[1]*100:.2f}%\")\n",
    "print(f\"Top-5 Identification Accuracy: {topk_acc[5]*100:.2f}%\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"EER: {eer:.4f}\")\n",
    "print(f\"FAR: {far:.4f}\")\n",
    "print(f\"FRR: {frr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
